import verifiers as vf
import vf_exts as vfe

from datasets import Dataset

import random
import os

import openai

various_nouns = [
    "cat",
    "dog",
    "bird",
    "fish",
    "horse",
    "rabbit",
    "snake",
    "tiger",
    "chair",
    "table",
    "book",
    "pen",
    "pencil",
    "eraser",
    "notebook",
    "computer",
    "program",
    "ai",
    "robot"
]

JUDGE_PROMPT = """
Below are two responses generated by a large language model. Please judge which response is better, according to the rubric provided.

Your final answer should be in the form of an integer between 1 and 7, inclusive.
A response of 1 means that Response A is much better than Response B, 7 means that Response B is much better than Response A, and 4 means that the two responses are equally good.

Return your answer in XML tags, like this:

<answer>4</answer>

Reply with no other text.

<rubric>
The poem should include as many markdown formatting elements as humanly possible.
</rubric>

<response_a>
{}
</response_a>

<response_b>
{}
</response_b>
"""

def load_environment(num_examples=100,seed=42) -> vf.MultiTurnEnv:
    random.seed(seed)
    dataset = Dataset.from_dict({
        "question": [f"Please write a poem about the following word: {random.choice(various_nouns)}" for i in range(num_examples)],
        "answer": [""] * num_examples,
    })
    
    openrouter_key = os.getenv("OPENROUTER_API_KEY")
    
    client = openai.OpenAI(api_key=openrouter_key, base_url="https://openrouter.ai/api/v1")
    
    rubric = vfe.PairwiseJudgeRubric(judge_client=client, judge_model="google/gemini-2.5-flash-lite", judge_prompt=JUDGE_PROMPT)
    
    env = vf.SingleTurnEnv(dataset=dataset, rubric=rubric)
    return env